import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
import numpy as np
import xarray as xr
import os
import pathlib
import joblib
from sklearn.preprocessing import StandardScaler



class ClimateDataset():
    """
    A custom dataset class for climate data and Aerosol Optical Depth (AOD) prediction.
    Args:
        var_names (list of str): Names of the climate input variables.
        *input_vars (np.ndarray): Variable-length list of input arrays. The last array is always AOD data; others are climate variables.
        model_save_path (str): Directory path to save scaler models.
        model_save_name (str): Filename for saving scaler models.
        fit_scalers (bool, optional): If True, fit new StandardScaler objects to input variables and save them. If False, use provided scalers. Default is True.
        **scalers: Pre-fitted scaler objects for each input variable, required if fit_scalers is False. -> fit scalers on the train set and save them to disk. Use pre-saved scalers for validation/test sets.
    Notes:
        - The last input variable is always treated as AOD data.
        - Optionally masks out low AOD data points (code commented out because it made the fit worse).
        - Scalers are saved to disk if fit_scalers is True.
        - Input variables are scaled using StandardScaler.
    """
    def __init__(self, var_names, *input_vars, model_save_path, model_save_name, fit_scalers=True, **scalers):
        self.var_names = var_names
        # Last input variable is always AOD
        self.aod_data = input_vars[-1]
        self.input_vars = list(input_vars[:-1])  # All others are climate inputs

        print(f"Unmasked input shapes: {[v.shape for v in self.input_vars]}, AOD shape: {self.aod_data.shape}")

        ## Mask out low AOD entries
        # mask = np.any(self.aod_data > 0.01, axis=1)
        # self.aod_data = self.aod_data[mask]
        # self.input_vars = [v[mask] for v in input_vars]

        print(f"Masked input shapes: {[v.shape for v in self.input_vars]}, AOD shape: {self.aod_data.shape}")
        
        # Prepare scalers
        self.scalers = {}
        out_dir = f'{model_save_path}{model_save_name[:-4]}'
        pathlib.Path(out_dir).mkdir(parents=True, exist_ok=True)

        if fit_scalers:
            for name, var in zip(self.var_names, self.input_vars):
                scaler = StandardScaler()
                scaled_var = scaler.fit_transform(var.reshape(-1, 1)).reshape(var.shape)
                self.input_vars[self.var_names.index(name)] = scaled_var
                self.scalers[name] = scaler

                joblib.dump(scaler, os.path.join(out_dir, f'{name}_scaler.pkl'))
                print(f"Scaler saved to {os.path.join(out_dir, f'{name}_scaler.pkl')}")
        else:
            for name, var in zip(self.var_names, self.input_vars):
                scaler_key = f'{name}_scaler'
                if scaler_key not in scalers:
                    raise ValueError(f"Missing scaler: {scaler_key}")
                scaler = scalers[scaler_key]
                scaled_var = scaler.transform(var.reshape(-1, 1)).reshape(var.shape)
                self.input_vars[self.var_names.index(name)] = scaled_var
                self.scalers[name] = scaler

        print(f"Final input variable shapes: {[v.shape for v in self.input_vars]}, AOD: {self.aod_data.shape}")


    def __len__(self):
        return self.input_vars[0].shape[0]

    def __getitem__(self, idx):
        x = np.stack([var[idx] for var in self.input_vars], axis=0)  # Shape: [channels, length]
        x = torch.tensor(x, dtype=torch.float32)
        y = torch.tensor(self.aod_data[idx], dtype=torch.float32).unsqueeze(0)
        return x, y


class UNet(nn.Module):
    """
    1D UNet architecture for sequence data.

    Args:
        in_channels (int): Number of input channels.
        out_channels (int, optional): Number of output channels. Default is 1.

    Architecture:
        - Encoder: Four convolutional blocks with increasing feature channels (32, 64, 128, 256),
          each followed by 1D max pooling for downsampling.
        - Decoder: Three convolutional blocks with skip connections from the encoder,
          using interpolation for upsampling and concatenation for feature fusion.
        - Final layer: 1D convolution to map features to the desired output channels.

    Forward Pass:
        - Input is processed through encoder blocks and pooled.
        - Decoder blocks upsample and concatenate corresponding encoder features (skip connections).
        - Output is generated by the final convolutional layer.

    Shape:
        - Input: (batch_size, in_channels, sequence_length)
        - Output: (batch_size, out_channels, sequence_length)
    """
    def __init__(self, in_channels, out_channels=1):
        super(UNet, self).__init__()

        def conv_block(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm1d(out_channels),
                nn.ReLU(),
                nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm1d(out_channels),
                nn.ReLU()
            )

        # Encoder
        self.enc1 = conv_block(in_channels, 32)
        self.enc2 = conv_block(32, 64)
        self.enc3 = conv_block(64, 128)
        self.enc4 = conv_block(128, 256)

        self.pool = nn.MaxPool1d(2)

        # Decoder with increased input channels to handle skip connections
        self.dec3 = conv_block(256 + 128, 128)
        self.dec2 = conv_block(128 + 64, 64)
        self.dec1 = conv_block(64 + 32, 32)

        self.final_conv = nn.Conv1d(32, out_channels, kernel_size=1)

    def forward(self, x):
        # Encoder
        e1 = self.enc1(x)                  # [B, 32, 128]
        e2 = self.enc2(self.pool(e1))     # [B, 64, 64]
        e3 = self.enc3(self.pool(e2))     # [B, 128, 32]
        e4 = self.enc4(self.pool(e3))     # [B, 256, 16]

        # Decoder with skip connections
        d3_up = torch.nn.functional.interpolate(e4, scale_factor=2, mode='linear', align_corners=False)  # [B, 256, 32]
        d3 = self.dec3(torch.cat([d3_up, e3], dim=1))  # [B, 128, 32]

        d2_up = torch.nn.functional.interpolate(d3, scale_factor=2, mode='linear', align_corners=False)  # [B, 128, 64]
        d2 = self.dec2(torch.cat([d2_up, e2], dim=1))  # [B, 64, 64]

        d1_up = torch.nn.functional.interpolate(d2, scale_factor=2, mode='linear', align_corners=False)  # [B, 64, 128]
        d1 = self.dec1(torch.cat([d1_up, e1], dim=1))  # [B, 32, 128]

        out = self.final_conv(d1)  # [B, 1, 128]
        return out



# Training function
def train_model(model, train_loader, val_loader, criterion, optimizer, model_save_path, model_save_name, num_epochs, patience=15):
    """    Train the model with early stopping and model saving.
    Parameters:
    -------
        model : torch.nn.Module
            The neural network model to train.
        train_loader : torch.utils.data.DataLoader
            DataLoader for the training dataset.
        val_loader : torch.utils.data.DataLoader
            DataLoader for the validation dataset.
        criterion : torch.nn.modules.loss._Loss
            Loss function used to evaluate the model.
        optimizer : torch.optim.Optimizer
            Optimization algorithm used to update model parameters.
        model_save_path : str
            Directory where the trained model and training log will be saved.
        model_save_name : str
            Filename to use when saving the best model (e.g., 'best_model.pth').
        num_epochs : int
            Total number of epochs to train the model.
        patience : int, optional
            Number of consecutive epochs with no improvement in validation loss 
            after which training will be stopped early (default is 15).

    Returns
    -------
    Nothing
        The function saves the best model to disk and logs the training and 
        validation loss per epoch to a text file.
    
    Notes
    -----
    - The model is saved only when the validation loss improves.
    - Early stopping is triggered if the validation loss does not improve for 
      'patience' consecutive epochs.
    - Training and validation losses are printed to the console and logged to a file.

    """
    import pathlib
    import matplotlib.pyplot as plt 
    import os

    train_losses, val_losses, accuracies = [], [], []
    best_val_loss = float('inf')
    patience_counter = 0

    # Create log file path
    log_dir = f'{model_save_path}{model_save_name[:-4]}'
    log_path = os.path.join(log_dir, 'training_log.txt')

    # Create directory if it doesn't exist
    pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)

    with open(log_path, 'w') as log_file:
        for epoch in range(num_epochs):
            model.train()
            train_loss = 0.0
            for input, aod in train_loader:
                optimizer.zero_grad()
                outputs = model(input)
                loss = criterion(outputs, aod)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()

            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for input, aod in val_loader:
                    outputs = model(input)
                    loss = criterion(outputs, aod)
                    val_loss += loss.item()

            train_loss_epoch = train_loss / len(train_loader)
            val_loss_epoch = val_loss / len(val_loader)

            log_line = f"Epoch {epoch+1}, Train Loss: {train_loss_epoch:.6f}, Val Loss: {val_loss_epoch:.6f}\n"
            print(log_line.strip())
            log_file.write(log_line)

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0 # Reset patience counter
                # Save model
                model_file_path = os.path.join(log_dir, model_save_name)
                pathlib.Path(model_file_path).unlink(missing_ok=True)
                torch.save(model.state_dict(), model_file_path)
                print(f"Model saved to {model_file_path}")
                log_file.write(f"Model saved to {model_file_path}\n")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print("Early stopping triggered.")
                    log_file.write("Early stopping triggered.\n")
                    break


    ###### Save trained model
    # create directory if it doesn't exist
    pathlib.Path(f'{model_save_path}{model_save_name[:-4]}').mkdir(parents=True, exist_ok=True)
    # delete exisiting file if exists
    pathlib.Path(f'{model_save_path}{model_save_name[:-4]}/{model_save_name}').unlink(missing_ok=True)
    # save
    torch.save(model.state_dict(), f'{model_save_path}{model_save_name[:-4]}/{model_save_name}')
    print(f"Model saved to {model_save_path}{model_save_name[:-4]}/{model_save_name}")


    # Plot training and validation loss
    # plt.figure()
    # plt.plot(range(1, num_epochs + 1), train_losses, marker='o', linestyle='-', label='Train Loss')
    # plt.plot(range(1, num_epochs + 1), val_losses, marker='o', linestyle='-', label='Val Loss')
    # plt.xlabel('Epoch')
    # plt.ylabel('Loss')
    # plt.title('Training and Validation Loss Over Epochs')
    # plt.legend()
    # plt.grid()
    # plt.savefig(f'{model_save_path}{model_save_name[:-4]}_loss.pdf', bbox_inches='tight')
    # plt.show()
    
    # # Plot accuracy
    # plt.figure()
    # plt.plot(range(1, num_epochs + 1), accuracies, marker='o', linestyle='-', label='Train Accuracy')
    # plt.xlabel('Epoch')
    # plt.ylabel('Mean Absolute Error')
    # plt.title('Training Accuracy Over Epochs')
    # plt.legend()
    # plt.grid()
    # plt.savefig(f'{model_save_path}{model_save_name[:-4]}_acc.pdf', bbox_inches='tight')
    # plt.show()

    
